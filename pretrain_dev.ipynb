{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "224bc54e-fbfc-43ff-af27-57ded4531bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PJRT_DEVICE=TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a29f10f-38b0-46b7-8982-304114a4d393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.8535, -0.0823, -0.2653],\n",
      "        [-0.0051,  1.3175, -0.5839],\n",
      "        [-1.2089,  1.2293,  1.3118]], device='xla:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "\n",
    "dev = xm.xla_device()\n",
    "t1 = torch.randn(3,3,device=dev)\n",
    "t2 = torch.randn(3,3,device=dev)\n",
    "print(t1 + t2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22fab52c-9d61-4c71-a21e-8ee0e5f501d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eb9085d-6863-4f96-aca2-dd0941112aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import evaluate\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    TrainingArguments,\n",
    "    TrainerState,\n",
    "    TrainerControl,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "from accelerate import Accelerator, DataLoaderConfiguration, DistributedType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa81d274-fa2d-433e-ab61-cb2c62b5c8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import IterableDataset, get_worker_info, DataLoader\n",
    "\n",
    "from dataset import StatefulShardedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b666a19f-008e-4a65-a5c5-4cff90ac59de",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_SEED = 42\n",
    "torch.manual_seed(TOP_SEED)\n",
    "\n",
    "base = '/home/shuyaoli/llm_data/converted_dataset'\n",
    "domain_dirs = {\n",
    "    'book':        os.path.join(base, 'book'),\n",
    "    'arxiv':       os.path.join(base, 'arxiv'),\n",
    "    'stackexchange':os.path.join(base, 'stackexchange'),\n",
    "    'wiki':        os.path.join(base, 'wiki'),\n",
    "    'c4-rp':       os.path.join(base, 'c4-rp'),\n",
    "    'cc':          os.path.join(base, 'cc'),\n",
    "    'github':      os.path.join(base, 'github'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76f4cfdf-9646-466d-b3d7-3cb91c063876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1ad7ea5-6f6a-4384-b420-959fcfebcd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicSamplingCallback(TrainerCallback): # Not used\n",
    "    \"\"\"\n",
    "    A Hugging Face TrainerCallback that dynamically adjusts dataset sampling\n",
    "    weights based on the training loss every N steps.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: StatefulShardedDataset,\n",
    "        update_every_n_steps: int = 100,\n",
    "        # Your logic to map loss to weights goes here\n",
    "        weight_update_fn: callable = lambda loss: [1.0 / max(loss, 1e-6)] \n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.update_every_n_steps = update_every_n_steps\n",
    "        self.weight_update_fn = weight_update_fn\n",
    "        self.running_losses = []\n",
    "\n",
    "    def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        \"\"\"Event called at the end of a training step.\"\"\"\n",
    "        # Get the latest loss from the trainer's state\n",
    "        # The log_history contains dicts like {'loss': 3.4, 'learning_rate':...}\n",
    "        if state.log_history:\n",
    "            latest_loss = state.log_history[-1].get(\"loss\")\n",
    "            if latest_loss is not None:\n",
    "                self.running_losses.append(torch.tensor(latest_loss, device=xm.xla_device()))\n",
    "\n",
    "        # Check if it's time to perform an update\n",
    "        if state.global_step > 0 and state.global_step % self.update_every_n_steps == 0:\n",
    "            if not self.running_losses:\n",
    "                return # Nothing to do if we haven't collected any losses\n",
    "\n",
    "            # This must be called on all processes to avoid deadlocks.\n",
    "            # It gathers the loss tensors from all TPU cores and averages them.\n",
    "            avg_loss_tensor = xm.mesh_reduce(\n",
    "                'loss_reduce_tag',\n",
    "                torch.mean(torch.stack(self.running_losses)),\n",
    "                lambda x: torch.mean(torch.stack(x))\n",
    "            )\n",
    "            # Clear the buffer for the next window\n",
    "            self.running_losses = []\n",
    "\n",
    "            # The rest of the logic should only run on the master process\n",
    "            if xm.is_master_ordinal():\n",
    "                avg_loss_val = avg_loss_tensor.item()\n",
    "                \n",
    "                # 1. Calculate new weights using the provided function\n",
    "                new_weights = self.weight_update_fn(avg_loss_val)\n",
    "                \n",
    "                # 2. Log the information\n",
    "                print(f\"\\n--- Step {state.global_step} ---\")\n",
    "                print(f\"Avg loss over last {self.update_every_n_steps} steps: {avg_loss_val:.4f}\")\n",
    "                print(f\"Updating sampling weights to: {[f'{w:.4f}' for w in new_weights]}\")\n",
    "                \n",
    "                # 3. Update the dataset's weights directly\n",
    "                self.dataset.update_weights(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c78528f-2774-4ae3-9e26-5c451bab024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicSamplingOnEvaluationCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A Hugging Face TrainerCallback that dynamically adjusts dataset sampling\n",
    "    weights based on the evaluation loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset: StatefulShardedDataset, weight_update_fn: callable):\n",
    "        self.dataset = dataset\n",
    "        self.weight_update_fn = weight_update_fn\n",
    "\n",
    "    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, metrics: dict[str, float], **kwargs):\n",
    "        \"\"\"Event called after an evaluation phase.\"\"\"\n",
    "        if xm.is_master_ordinal():\n",
    "            eval_loss = metrics.get(\"eval_loss\")\n",
    "            if eval_loss is None:\n",
    "                print(\"Warning: 'eval_loss' not found in metrics. Skipping weight update.\")\n",
    "                return\n",
    "\n",
    "            new_weights = self.weight_update_fn(eval_loss)\n",
    "            print(f\"\\n--- Evaluation at Step {state.global_step} ---\")\n",
    "            print(f\"Evaluation Loss: {eval_loss:.4f}\")\n",
    "            print(f\"Updating sampling weights to: {[f'{w:.4f}' for w in new_weights]}\")\n",
    "            self.dataset.update_weights(new_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d1c99e4-0d08-40b2-a05e-4d1109d93b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    A custom Trainer that overrides the training dataloader to use\n",
    "    our streaming dataset and the PyTorch/XLA MpDeviceLoader.\n",
    "    \"\"\"\n",
    "    pass\n",
    "    # def get_train_dataloader(self) -> DataLoader:\n",
    "    #     if self.train_dataset is None:\n",
    "    #         raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "        \n",
    "    #     assert isinstance(self.train_dataset, StatefulStreamingDataset), \\\n",
    "    #         \"train_dataset must be an instance of StatefulStreamingDataset\"\n",
    "        \n",
    "    #     # Use the PyTorch/XLA ParallelLoader for TPUs\n",
    "    #     return pl.MpDeviceLoader(\n",
    "    #         self.train_dataset,\n",
    "    #         device=self.args.device,\n",
    "    #         batch_size=self.args.per_device_train_batch_size,\n",
    "    #         num_workers=self.args.dataloader_num_workers,\n",
    "    #         worker_init_fn=worker_init_fn,\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "655e601d-a4ce-49cb-9aed-d6d2ce29e6f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StreamingDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m FINAL_EVAL_BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      2\u001b[0m EVAL_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/shuyaoli/llm_data/LLM-Shearing/for_prune/eval_merge\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mStreamingDataset\u001b[49m(\n\u001b[1;32m      4\u001b[0m     local\u001b[38;5;241m=\u001b[39mEVAL_PATH,\n\u001b[1;32m      5\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mFINAL_EVAL_BATCH_SIZE\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StreamingDataset' is not defined"
     ]
    }
   ],
   "source": [
    "FINAL_EVAL_BATCH_SIZE = 8\n",
    "EVAL_PATH = '/home/shuyaoli/llm_data/LLM-Shearing/for_prune/eval_merge'\n",
    "eval_dataset = StreamingDataset(\n",
    "    local=EVAL_PATH,\n",
    "    batch_size=FINAL_EVAL_BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8aaff8f0-a40b-4dd7-b68e-df01544d763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 8\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "882968a8-e02e-40d2-89af-5fe9c38e3d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized dataset with 7 domains.\n",
      "  -> Found 7 shards for domain 'book'\n",
      "  -> Found 4 shards for domain 'arxiv'\n",
      "  -> Found 4 shards for domain 'stackexchange'\n",
      "  -> Found 7 shards for domain 'wiki'\n",
      "  -> Found 23 shards for domain 'c4-rp'\n",
      "  -> Found 103 shards for domain 'cc'\n",
      "  -> Found 7 shards for domain 'github'\n"
     ]
    }
   ],
   "source": [
    "initial_weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "\n",
    "# Instantiate your master dataset\n",
    "master_train_dataset = StatefulShardedDataset(\n",
    "    domain_dirs=domain_dirs,\n",
    "    initial_weights=initial_weights,\n",
    "    chunk_size=4  # small for demo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1ac683d1-8a92-4e83-9256-183fe8dc3a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Sheared-LLaMA model for continued pretraining.\n",
    "model_name = \"princeton-nlp/Sheared-LLaMA-1.3B-Pruned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b773384e-7f3f-4bed-9e8b-ede5ddfad10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need to be logged into your Hugging Face account and have\n",
    "# access to meta-llama models for this to work.\n",
    "# `huggingface-cli login`\n",
    "tokenizer_name = \"meta-llama/Llama-2-7b-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8ca1cbaf-72e5-4910-bf40-07a955881130",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "# Set pad token to EOS token for Causal LM\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "df537da5-5d61-4f6b-8180-f4bfc371ecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tpu_eval_sampling_model\",\n",
    "    max_steps=5000,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_steps=100,\n",
    "    \n",
    "    # --- Crucial for this strategy ---\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=128, # Run evaluation every 1000 steps\n",
    "    # ---------------------------------\n",
    "    \n",
    "    dataloader_num_workers=1, # At least 1 worker for worker_init_fn to be called\n",
    "    remove_unused_columns=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4fdd939a-5b4f-4319-85b9-4925896803a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_sampling_callback = DynamicSamplingOnEvaluationCallback(\n",
    "    dataset=master_train_dataset,\n",
    "    weight_update_fn=lambda x: 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "08f52c6b-5c78-4da3-b5fa-22076beed006",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = StreamingTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=master_train_dataset,\n",
    "    eval_dataset=eval_dataset, # Provide the evaluation dataset\n",
    "    callbacks=[eval_sampling_callback], # Use the new callback\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "52eab4c4-cef5-49c9-a140-513c2b96e5f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-22 (_loader_worker):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/shuyaoli/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shuyaoli/.local/lib/python3.10/site-packages/torch_xla/distributed/parallel_loader.py\", line 165, in _loader_worker\n",
      "    _, data = next(data_iter)\n",
      "  File \"/home/shuyaoli/.local/lib/python3.10/site-packages/accelerate/data_loader.py\", line 564, in __iter__\n",
      "    dataloader_iter = self.base_dataloader.__iter__()\n",
      "  File \"/home/shuyaoli/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 493, in __iter__\n",
      "    return self._get_iterator()\n",
      "  File \"/home/shuyaoli/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 424, in _get_iterator\n",
      "    return _MultiProcessingDataLoaderIter(self)\n",
      "  File \"/home/shuyaoli/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1171, in __init__\n",
      "    w.start()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 121, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/context.py\", line 224, in _Popen\n",
      "    return _default_context.get_context().Process._Popen(process_obj)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/context.py\", line 288, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n",
      "    super().__init__(process_obj)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/popen_fork.py\", line 19, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\n",
      "    reduction.dump(process_obj, fp)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/reduction.py\", line 60, in dump\n",
      "    ForkingPickler(file, protocol).dump(obj)\n",
      "_pickle.PicklingError: Can't pickle <class 'dataset.StatefulStreamingDataset'>: it's not the same object as dataset.StatefulStreamingDataset\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch_xla/csrc/xla_graph_executor.cpp:689 : Check failed: tensor_data \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace[abi:cxx11]()\n\ttorch_xla::XLAGraphExecutor::CollectSyncTensors(std::vector<c10::intrusive_ptr<torch_xla::XLATensor, c10::detail::intrusive_target_default_null_type<torch_xla::XLATensor> >, std::allocator<c10::intrusive_ptr<torch_xla::XLATensor, c10::detail::intrusive_target_default_null_type<torch_xla::XLATensor> > > > const&, torch::lazy::LazyGraphExecutor::SyncTensorsConfig const&)\n\ttorch_xla::XLAGraphExecutor::SyncTensorsGraphInternal(std::vector<c10::intrusive_ptr<torch_xla::XLATensor, c10::detail::intrusive_target_default_null_type<torch_xla::XLATensor> >, std::allocator<c10::intrusive_ptr<torch_xla::XLATensor, c10::detail::intrusive_target_default_null_type<torch_xla::XLATensor> > > >*, absl::lts_20230802::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const>, torch::lazy::LazyGraphExecutor::SyncTensorsConfig const&, bool)\n\ttorch_xla::XLAGraphExecutor::SyncTensorsGraph(std::vector<c10::intrusive_ptr<torch_xla::XLATensor, c10::detail::intrusive_target_default_null_type<torch_xla::XLATensor> >, std::allocator<c10::intrusive_ptr<torch_xla::XLATensor, c10::detail::intrusive_target_default_null_type<torch_xla::XLATensor> > > >*, absl::lts_20230802::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const>, bool, bool, bool)\n\ttorch_xla::XLAGraphExecutor::SyncLiveTensorsGraph(torch::lazy::BackendDevice const*, c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, bool)\n\t\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\tPyObject_Call\n\t\n\t_PyObject_MakeTpCall\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t\n\tPy_RunMain\n\tPy_BytesMain\n\t\n\t__libc_start_main\n*** End stack trace ***\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2206\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2207\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2502\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2500\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2501\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[0;32m-> 2502\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[1;32m   2504\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:5300\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches, device)\u001b[0m\n\u001b[1;32m   5298\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m   5299\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5300\u001b[0m         batch_samples\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   5301\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   5302\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_xla/distributed/parallel_loader.py:34\u001b[0m, in \u001b[0;36mPerDeviceLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 34\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_xla/distributed/parallel_loader.py:46\u001b[0m, in \u001b[0;36mPerDeviceLoader.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mark_step_batch_count \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batches_yielded:\n\u001b[1;32m     45\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batches_yielded \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 46\u001b[0m   \u001b[43mxm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmark_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batches_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_xla/core/xla_model.py:1051\u001b[0m, in \u001b[0;36mmark_step\u001b[0;34m(wait, reset_scope)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xu\u001b[38;5;241m.\u001b[39mgetenv_as(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXLA_EMIT_STEPLOG\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1046\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m   1047\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch_xla.core.xla_model::mark_step\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1048\u001b[0m       end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1049\u001b[0m       file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1050\u001b[0m       flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 1051\u001b[0m \u001b[43mtorch_xla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_XLAC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_xla_step_marker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_xla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_XLAC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_xla_get_default_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv_as\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mXLA_SYNC_WAIT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset_scope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_scope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;66;03m# Only emit metrics from the first local device index, to avoid emitting the\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# same values from different threads.\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_master_ordinal():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch_xla/csrc/xla_graph_executor.cpp:689 : Check failed: tensor_data \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace[abi:cxx11]()\n\ttorch_xla::XLAGraphExecutor::CollectSyncTensors(std::vector<c10::intrusive_ptr<torch_xla::XLATensor, c10::detail::intrusive_target_default_null_type<torch_xla::XLATensor> >, std::allocator<c10::intrusive_ptr<torch_xla::XLATensor, c10::detail::intrusive_target_default_null_type<torch_xla::XLATensor> > > > const&, torch::lazy::LazyGraphExecutor::SyncTensorsConfig const&)\n\ttorch_xla::XLAGraphExecutor::SyncTensorsGraphInternal(std::vector<c10::intrusive_ptr<torch_xla::XLATensor, c10::detail::intrusive_target_default_null_type<torch_xla::XLATensor> >, std::allocator<c10::intrusive_ptr<torch_xla::XLATensor, c10::detail::intrusive_target_default_null_type<torch_xla::XLATensor> > > >*, absl::lts_20230802::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const>, torch::lazy::LazyGraphExecutor::SyncTensorsConfig const&, bool)\n\ttorch_xla::XLAGraphExecutor::SyncTensorsGraph(std::vector<c10::intrusive_ptr<torch_xla::XLATensor, c10::detail::intrusive_target_default_null_type<torch_xla::XLATensor> >, std::allocator<c10::intrusive_ptr<torch_xla::XLATensor, c10::detail::intrusive_target_default_null_type<torch_xla::XLATensor> > > >*, absl::lts_20230802::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const>, bool, bool, bool)\n\ttorch_xla::XLAGraphExecutor::SyncLiveTensorsGraph(torch::lazy::BackendDevice const*, c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, bool)\n\t\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\tPyObject_Call\n\t\n\t_PyObject_MakeTpCall\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t\n\tPy_RunMain\n\tPy_BytesMain\n\t\n\t__libc_start_main\n*** End stack trace ***\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de46ae82-2608-49c5-85c1-23a255fbf879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; cpu_count = os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72ea3941-b3ee-4dd2-b20a-b04e0be8ea40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "524f4c8b-3a7e-4f2d-b543-2fdc80f0f84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
     ]
    }
   ],
   "source": [
    "import torch_xla.core.xla_model as xm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6240422-fde5-4602-be02-5123cae7e4c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch_xla.core.xla_model' has no attribute 'xrt_world_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m world_size \u001b[38;5;241m=\u001b[39m \u001b[43mxm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxrt_world_size\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch_xla.core.xla_model' has no attribute 'xrt_world_size'"
     ]
    }
   ],
   "source": [
    "world_size = xm.xrt_world_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24159386-0ff8-47fc-9ece-dfba0279e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_xla.runtime as xr; world_size = xr.world_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76c53eb3-89ba-4ac1-8ffa-efd3bdde73fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab63ef7-7e5f-49eb-b05e-a8a168947ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
