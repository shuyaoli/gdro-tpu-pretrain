{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "224bc54e-fbfc-43ff-af27-57ded4531bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PJRT_DEVICE=TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a29f10f-38b0-46b7-8982-304114a4d393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.8535, -0.0823, -0.2653],\n",
      "        [-0.0051,  1.3175, -0.5839],\n",
      "        [-1.2089,  1.2293,  1.3118]], device='xla:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "dev = xm.xla_device()\n",
    "t1 = torch.randn(3,3,device=dev)\n",
    "t2 = torch.randn(3,3,device=dev)\n",
    "print(t1 + t2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22fab52c-9d61-4c71-a21e-8ee0e5f501d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffa88e9d-95de-4d3f-9b70-1a7d901d8774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eb9085d-6863-4f96-aca2-dd0941112aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import evaluate\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    TrainingArguments,\n",
    "    TrainerState,\n",
    "    TrainerControl,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "from accelerate import Accelerator, DataLoaderConfiguration, DistributedType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa81d274-fa2d-433e-ab61-cb2c62b5c8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from torch.utils.data import IterableDataset, get_worker_info\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    \"\"\"Initializes each data loader worker to have its own data streams.\"\"\"\n",
    "    worker_info = get_worker_info()\n",
    "    dataset = worker_info.dataset\n",
    "    # Each worker gets its own generator for each data source\n",
    "    dataset.data_streams = [iter(source) for source in dataset.sources]\n",
    "\n",
    "class StatefulStreamingDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    An IterableDataset that samples from multiple data sources based on\n",
    "    dynamically updatable weights, optimized for batched sampling.\n",
    "    \"\"\"\n",
    "    def __init__(self, sources, initial_weights, chunk_size=4096):\n",
    "        self.sources = sources\n",
    "        # Use a multiprocessing Array for weights to ensure they are shared\n",
    "        # across all data loader worker processes.\n",
    "        self.weights = mp.Array('d', initial_weights)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.data_streams = None\n",
    "\n",
    "    def _get_weights(self):\n",
    "        \"\"\"Reads the current weights from the shared memory array.\"\"\"\n",
    "        return np.array(self.weights[:])\n",
    "\n",
    "    def update_weights(self, new_weights: list[float]):\n",
    "        \"\"\"\n",
    "        Updates the shared weights. This method is called from the main\n",
    "        process by a custom callback.\n",
    "        \"\"\"\n",
    "        with self.weights.get_lock():\n",
    "            for i in range(len(new_weights)):\n",
    "                self.weights[i] = new_weights[i]\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"The core streaming logic for each worker, using chunked sampling.\"\"\"\n",
    "        # Initialize streams if they haven't been, relevant for num_workers=0\n",
    "        if self.data_streams is None:\n",
    "            self.data_streams = [iter(source) for source in self.sources]\n",
    "\n",
    "        while True:\n",
    "            # --- Start of Optimized Block ---\n",
    "            # 1. Get weights and probabilities ONCE per chunk.\n",
    "            current_weights = self._get_weights()\n",
    "            probabilities = current_weights / np.sum(current_weights)\n",
    "            \n",
    "            # 2. Generate a large chunk of source indices at once.\n",
    "            # This is vastly more efficient than calling it in a loop.\n",
    "            source_indices = np.random.choice(\n",
    "                len(self.sources), \n",
    "                size=self.chunk_size, \n",
    "                p=probabilities\n",
    "            )\n",
    "\n",
    "            # 3. Iterate through the pre-sampled chunk and yield items.\n",
    "            for source_idx in source_indices:\n",
    "                try:\n",
    "                    yield next(self.data_streams[source_idx])\n",
    "                except StopIteration:\n",
    "                    # When a source is exhausted, restart it for continuous training\n",
    "                    print(f\"Worker {get_worker_info().id if get_worker_info() else 0}: Restarting stream {source_idx}.\")\n",
    "                    self.data_streams[source_idx] = iter(self.sources[source_idx])\n",
    "                    yield next(self.data_streams[source_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e53266f-e945-4c29-b216-b5e6eb356224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from streaming import Stream, StreamingDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b666a19f-008e-4a65-a5c5-4cff90ac59de",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = '/home/shuyaoli/llm_data/LLM-Shearing/for_prune/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "589dbdc7-983f-4284-ac51-d17829b255d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing sources...\n",
      "Created 7 data sources.\n",
      "\n",
      "Starting DataLoader iteration...\n",
      "Step 0:\n",
      "  Batch keys: dict_keys(['set', 'tokens'])\n",
      "  Tokens tensor length: [8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192]\n",
      "  Domain counts in batch: {np.str_('arxiv'): np.int64(13), np.str_('book'): np.int64(19), np.str_('c4-rp'): np.int64(18), np.str_('cc'): np.int64(24), np.str_('github'): np.int64(15), np.str_('stackexchange'): np.int64(21), np.str_('wiki'): np.int64(18)}\n",
      "Step 1:\n",
      "  Batch keys: dict_keys(['set', 'tokens'])\n",
      "  Tokens tensor length: [8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192]\n",
      "  Domain counts in batch: {np.str_('arxiv'): np.int64(14), np.str_('book'): np.int64(21), np.str_('c4-rp'): np.int64(20), np.str_('cc'): np.int64(28), np.str_('github'): np.int64(13), np.str_('stackexchange'): np.int64(13), np.str_('wiki'): np.int64(19)}\n",
      "Step 2:\n",
      "  Batch keys: dict_keys(['set', 'tokens'])\n",
      "  Tokens tensor length: [8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192]\n",
      "  Domain counts in batch: {np.str_('arxiv'): np.int64(25), np.str_('book'): np.int64(18), np.str_('c4-rp'): np.int64(10), np.str_('cc'): np.int64(20), np.str_('github'): np.int64(13), np.str_('stackexchange'): np.int64(23), np.str_('wiki'): np.int64(19)}\n",
      "Step 3:\n",
      "  Batch keys: dict_keys(['set', 'tokens'])\n",
      "  Tokens tensor length: [8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192, 8192]\n",
      "  Domain counts in batch: {np.str_('arxiv'): np.int64(14), np.str_('book'): np.int64(22), np.str_('c4-rp'): np.int64(16), np.str_('cc'): np.int64(15), np.str_('github'): np.int64(20), np.str_('stackexchange'): np.int64(19), np.str_('wiki'): np.int64(22)}\n",
      "\n",
      "Demonstration complete.\n"
     ]
    }
   ],
   "source": [
    "!python3 dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76f4cfdf-9646-466d-b3d7-3cb91c063876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1ad7ea5-6f6a-4384-b420-959fcfebcd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicSamplingCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A Hugging Face TrainerCallback that dynamically adjusts dataset sampling\n",
    "    weights based on the training loss every N steps.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: StatefulStreamingDataset,\n",
    "        update_every_n_steps: int = 100,\n",
    "        # Your logic to map loss to weights goes here\n",
    "        weight_update_fn: callable = lambda loss: [1.0 / max(loss, 1e-6)] \n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.update_every_n_steps = update_every_n_steps\n",
    "        self.weight_update_fn = weight_update_fn\n",
    "        self.running_losses = []\n",
    "\n",
    "    def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        \"\"\"Event called at the end of a training step.\"\"\"\n",
    "        # Get the latest loss from the trainer's state\n",
    "        # The log_history contains dicts like {'loss': 3.4, 'learning_rate':...}\n",
    "        if state.log_history:\n",
    "            latest_loss = state.log_history[-1].get(\"loss\")\n",
    "            if latest_loss is not None:\n",
    "                self.running_losses.append(torch.tensor(latest_loss, device=xm.xla_device()))\n",
    "\n",
    "        # Check if it's time to perform an update\n",
    "        if state.global_step > 0 and state.global_step % self.update_every_n_steps == 0:\n",
    "            if not self.running_losses:\n",
    "                return # Nothing to do if we haven't collected any losses\n",
    "\n",
    "            # This must be called on all processes to avoid deadlocks.\n",
    "            # It gathers the loss tensors from all TPU cores and averages them.\n",
    "            avg_loss_tensor = xm.mesh_reduce(\n",
    "                'loss_reduce_tag',\n",
    "                torch.mean(torch.stack(self.running_losses)),\n",
    "                lambda x: torch.mean(torch.stack(x))\n",
    "            )\n",
    "            # Clear the buffer for the next window\n",
    "            self.running_losses = []\n",
    "\n",
    "            # The rest of the logic should only run on the master process\n",
    "            if xm.is_master_ordinal():\n",
    "                avg_loss_val = avg_loss_tensor.item()\n",
    "                \n",
    "                # 1. Calculate new weights using the provided function\n",
    "                new_weights = self.weight_update_fn(avg_loss_val)\n",
    "                \n",
    "                # 2. Log the information\n",
    "                print(f\"\\n--- Step {state.global_step} ---\")\n",
    "                print(f\"Avg loss over last {self.update_every_n_steps} steps: {avg_loss_val:.4f}\")\n",
    "                print(f\"Updating sampling weights to: {[f'{w:.4f}' for w in new_weights]}\")\n",
    "                \n",
    "                # 3. Update the dataset's weights directly\n",
    "                self.dataset.update_weights(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c78528f-2774-4ae3-9e26-5c451bab024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicSamplingOnEvaluationCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A Hugging Face TrainerCallback that dynamically adjusts dataset sampling\n",
    "    weights based on the evaluation loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset: StatefulStreamingDataset, weight_update_fn: callable):\n",
    "        self.dataset = dataset\n",
    "        self.weight_update_fn = weight_update_fn\n",
    "\n",
    "    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, metrics: dict[str, float], **kwargs):\n",
    "        \"\"\"Event called after an evaluation phase.\"\"\"\n",
    "        if xm.is_master_ordinal():\n",
    "            eval_loss = metrics.get(\"eval_loss\")\n",
    "            if eval_loss is None:\n",
    "                print(\"Warning: 'eval_loss' not found in metrics. Skipping weight update.\")\n",
    "                return\n",
    "\n",
    "            new_weights = self.weight_update_fn(eval_loss)\n",
    "            print(f\"\\n--- Evaluation at Step {state.global_step} ---\")\n",
    "            print(f\"Evaluation Loss: {eval_loss:.4f}\")\n",
    "            print(f\"Updating sampling weights to: {[f'{w:.4f}' for w in new_weights]}\")\n",
    "            self.dataset.update_weights(new_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d1c99e4-0d08-40b2-a05e-4d1109d93b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    A custom Trainer that overrides the training dataloader to use\n",
    "    our streaming dataset and the PyTorch/XLA MpDeviceLoader.\n",
    "    \"\"\"\n",
    "    def get_train_dataloader(self) -> DataLoader:\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "        \n",
    "        assert isinstance(self.train_dataset, StatefulStreamingDataset), \\\n",
    "            \"train_dataset must be an instance of StatefulStreamingDataset\"\n",
    "        \n",
    "        # Use the PyTorch/XLA ParallelLoader for TPUs\n",
    "        return pl.MpDeviceLoader(\n",
    "            self.train_dataset,\n",
    "            device=self.args.device,\n",
    "            batch_size=self.args.per_device_train_batch_size,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "            worker_init_fn=worker_init_fn,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "655e601d-a4ce-49cb-9aed-d6d2ce29e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_PATH = '/home/shuyaoli/llm_data/LLM-Shearing/for_prune/eval_merge'\n",
    "eval_dataset = StreamingDataset(\n",
    "    local=EVAL_PATH,\n",
    "    batch_size=FINAL_EVAL_BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8aaff8f0-a40b-4dd7-b68e-df01544d763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 128\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "882968a8-e02e-40d2-89af-5fe9c38e3d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing sources...\n",
      "Created 7 data sources.\n"
     ]
    }
   ],
   "source": [
    "mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "# --- Setup the Underlying Readers ---\n",
    "local_path = '/home/shuyaoli/llm_data/LLM-Shearing/for_prune'\n",
    "stream_names = [\n",
    "    'book', 'arxiv', 'stackexchange', 'wiki', 'c4-rp', 'cc', 'github'\n",
    "]\n",
    "\n",
    "print(\"Initializing sources...\")\n",
    "# Create a list of sources, where each source is a StreamingDataset for one domain\n",
    "sources = []\n",
    "for name in stream_names:\n",
    "    # Each StreamingDataset object is an independent iterable data source\n",
    "    domain_dataset = StreamingDataset(\n",
    "        local=os.path.join(local_path, name),\n",
    "        shuffle=True, # Shuffle within this stream\n",
    "        batch_size=TRAIN_BATCH_SIZE # We will handle batching in the final DataLoader\n",
    "    )\n",
    "    sources.append(domain_dataset)\n",
    "\n",
    "print(f\"Created {len(sources)} data sources.\")\n",
    "\n",
    "# --- Setup Your Custom Wrapper Dataset ---\n",
    "# Define the initial sampling weights for each stream\n",
    "# Make sure the length matches the number of streams!\n",
    "initial_weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "assert len(initial_weights) == len(sources)\n",
    "\n",
    "# Instantiate your master dataset\n",
    "master_dataset = StatefulStreamingDataset(\n",
    "    sources=sources,\n",
    "    initial_weights=initial_weights,\n",
    "    chunk_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac683d1-8a92-4e83-9256-183fe8dc3a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
